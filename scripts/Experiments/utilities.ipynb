{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c784110",
   "metadata": {},
   "source": [
    "### Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abfdef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "import classla\n",
    "import json\n",
    "import re\n",
    "from numpy import array, argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import model_from_json\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709c088e",
   "metadata": {},
   "source": [
    "### General Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0c678b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - name of a text file with one sentence per line\n",
    "# output - list of sentences (strings)\n",
    "def read_file_as_list_of_sentences(input_file_name):\n",
    "    with open(input_file_name, \"r\") as input_file:\n",
    "        return input_file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af982c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inpit - list of strings\n",
    "# output - list of strings\n",
    "def add_special_symbol_at_start_of_each_sentence(data):\n",
    "    return ['^{0}'.format(sentence) for sentence in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3669e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - list of sentences (strings) and a classla pipeline (POS tokenize or tokenize)\n",
    "# output - list of dictionaries ((POS) tokenized sentences) \n",
    "def run_through_classla_pipeline(list_of_sentences, pipeline):\n",
    "    return [pipeline(sentence).to_dict()[0][0] for sentence in list_of_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "651555ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - list of dictionaries (a dictionary for each word - the tokenized version of the sentence)\n",
    "# output - list of dictionaries (dictionaries that contain punctuation one after another are squashed into one)\n",
    "def squash_punctuation(sentence):\n",
    "    new_sentence = []\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i]['text'] in ',()\";.?!:-':\n",
    "            if len(new_sentence) > 0 and all(character in ',()\";.?!:-' for character in new_sentence[-1]['text']):\n",
    "                new_sentence[-1]['text'] += sentence[i]['text']\n",
    "            else:\n",
    "                new_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_sentence.append(sentence[i])\n",
    "            \n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42da702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - label (string)\n",
    "# output - transformed label (string) - one that is part of the allowed labels\n",
    "def transform_label(label):\n",
    "    label_priorities = {1: ',', 2: '?', 3: '!', 4: '.', 5: ':',\n",
    "                        6: '-', 7: ';', 8: '(', 9: ')', 10: '\"',\n",
    "                        11: '...'}\n",
    "    \n",
    "    allowed_labels = [',', '?', '!', '.', ':', '-', ';', '(', ')', '\"',\n",
    "                      '', '...', '\",', '),', '\".', ').', ':\"']\n",
    "    \n",
    "    matched_label = next((allowed_label for allowed_label in allowed_labels[11:] if allowed_label in label),\n",
    "                         False)\n",
    "\n",
    "    \n",
    "    if label in allowed_labels:\n",
    "        return label\n",
    "    elif matched_label:\n",
    "        return matched_label\n",
    "    else:\n",
    "        for i in range(1, 12):\n",
    "            if label_priorities[i] in label:\n",
    "                return label_priorities[i]\n",
    "    \n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5274e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence and an index of a word (dictionary) - assigns a label to each word based on the punctuation after\n",
    "# output - a label - None for punctuation, punctuation for words followed by punctuation and empty for words if they are not\n",
    "def word2label(sentence, i):\n",
    "    if all(character in ',()\";.?!:-' for character in sentence[i]['text']):\n",
    "        return None\n",
    "\n",
    "    if i < len(sentence) - 1:\n",
    "        if all(character in ',()\";.?!:-' for character in sentence[i+1]['text']):\n",
    "            label = transform_label(sentence[i+1]['text'])\n",
    "            return label\n",
    "    \n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d73191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries)\n",
    "# output - list of labels (strings) with None labels for punctuation being filtered out\n",
    "def sent2labels(sentence):\n",
    "    return [label for label in (word2label(sentence, i) for i in range(len(sentence))) if label != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "957a8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - list of dictionaries (a dictionary for each word - the tokenized version of the sentence)\n",
    "# output - new sentence (string) with the punctuation removed\n",
    "def remove_punctuation(sentence):\n",
    "    new_sentence = ''\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        if all(character in ',()\";.?!:-' for character in sentence[i]['text']):\n",
    "            pass\n",
    "        else:\n",
    "            new_sentence = new_sentence + sentence[i]['text'] + ' '\n",
    "    \n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7fef0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries)\n",
    "# output - True/False depending on whether the sentence contains interrogative word or not\n",
    "def contains_interrogative_word(sentence):\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i]['xpos'].startswith('Pi'):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65110e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries)\n",
    "# output - True/False depending on whether the sentence contains interrogative particle or not\n",
    "def contains_interrogative_particle(sentence):\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i]['xpos'] == 'Ti':\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc80bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries)\n",
    "# output - True/False depending on whether the sentence contains imperative verb or not\n",
    "def contains_imperative_verb(sentence):\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i]['xpos'][0] == 'V' and sentence[i]['xpos'][4] == 'z':\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ef93cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries) and index of a word (dictionary)\n",
    "# output - True/False depending on whether the sentence contains relative pronoun before and the tag\n",
    "#          of the pronoun in the sentence (returns '' if False)\n",
    "def contains_relative_pronoun_before(sentence, i):\n",
    "    for word_i in reversed(range(len(sentence[:i]))):\n",
    "        if sentence[word_i]['xpos'].startswith('Pr'):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "244b362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries) and index of a word (dictionary)\n",
    "# output - True/False depending on whether the sentence contains repetitive conjunction before\n",
    "def contains_rep_conj_before(sentence, i):\n",
    "    if i < len(sentence)-1 and sentence[i+1]['xpos'] in ('Cr', 'Cp'):\n",
    "        for word_i in reversed(range(len(sentence[:i]))):\n",
    "            if sentence[word_i]['xpos'] == sentence[i+1]['xpos']:\n",
    "                return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d372051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries) and index of a word (dictionary)\n",
    "# output - True/False depending on whether the sentence contains repetitive word before and the tag\n",
    "#          of the word in the sentence (returns '' if False)\n",
    "def contains_repetitive_word_before(sentence, i):\n",
    "    for word_i in reversed(range(len(sentence[:i]))):\n",
    "        if i < len(sentence)-1 and sentence[word_i]['text'].lower() == sentence[i+1]['text'].lower():\n",
    "            return True, sentence[word_i]['xpos']\n",
    "    \n",
    "    return False, ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ca6a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries)\n",
    "# output - True/False depending on whether the sentence contains publicistic word\n",
    "def contains_publicistic_word(sentence):\n",
    "    publicistic_words_list = ['казва', 'каза', 'заяви', 'заявява', 'отбеляза', 'добави', 'добавя',\n",
    "                              'цитира', 'твърди', 'обясни', 'посочи', 'допълни', 'подчерта', 'писа',\n",
    "                              'изтъкна', 'посочва', 'пише', 'заключи', 'сподели', 'обяснява', 'предупреди',\n",
    "                              'отбелязва', 'предупреждава', 'призова', 'допълва', 'съобщи', 'заяви', 'обяви',\n",
    "                              'коментира']\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i]['text'] in publicistic_words_list:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c51ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries) and index of a word (dictionary)\n",
    "# output - the count of the verbs before the word\n",
    "def count_of_verbs_before(sentence, i):\n",
    "    verbs_count = 0\n",
    "\n",
    "    for word_i in range(len(sentence[:i])):\n",
    "        if sentence[word_i]['upos'] == 'VERB':\n",
    "            verbs_count += 1\n",
    "    \n",
    "    return verbs_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9ee2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries) and index of a word (dictionary)\n",
    "# output - the count of the verbs before the word\n",
    "def between_two_verbs(sentence, i):\n",
    "    verb_before = False\n",
    "    verb_after = False\n",
    "\n",
    "    for word_i in reversed(range(len(sentence[:(i+1)]))):\n",
    "        if sentence[word_i]['upos'] == 'VERB':\n",
    "            verb_before = True\n",
    "    \n",
    "    for word in sentence[(i+1):]:\n",
    "        if word['upos'] == 'VERB':\n",
    "            verb_after = True\n",
    "    \n",
    "    return verb_before and verb_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4cc7461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - a full xpos tag as defined in BulTreeBank tagset (string) and a prefix word for the output dictionary keys (string)\n",
    "# output - a dictionary with subtags generated from the full xpos tag with prefix word and their corresponding values\n",
    "def split_xpos(xpos, word='word'):\n",
    "    def gender_number_article(number, gender, article):\n",
    "        gender_number_article = ''\n",
    "        \n",
    "        for feature in (gender, number, article):\n",
    "            if feature:\n",
    "                gender_number_article += feature\n",
    "            else:\n",
    "                gender_number_article += '-'\n",
    "        \n",
    "        return gender_number_article\n",
    "\n",
    "    pos2features = {'N': {'xpos_type': xpos[:2], 'xpos_gender_number_article': xpos[2:5]},\n",
    "                    'A': {'xpos_type': xpos[:1], 'xpos_gender_number_article': xpos[1:4]},\n",
    "                    'H': {'xpos_type': xpos[:1], 'xpos_gender_number_article': xpos[1:4]},\n",
    "                    'M': {'xpos_type': xpos[:2], 'xpos_gender_number_article': xpos[2:5]},\n",
    "                    'V': {'xpos_type': xpos[:2],\n",
    "                          'xpos_gender_number_article': gender_number_article(xpos[8:9], xpos[9:10], xpos[10:11])},\n",
    "                    'P': {'xpos_type': xpos[:2],\n",
    "                          'xpos_gender_number_article': gender_number_article(xpos[5:6], xpos[7:8], xpos[8:9])},\n",
    "                    'D': {'xpos_type': xpos},\n",
    "                    'C': {'xpos_type': xpos},\n",
    "                    'T': {'xpos_type': xpos},\n",
    "                    'R': {'xpos_type': xpos},\n",
    "                    'I': {'xpos_type': xpos}}\n",
    "    \n",
    "    result_pos2features = pos2features.get(xpos[0], {'xpos_type': xpos})\n",
    "    \n",
    "    if xpos[0] == 'V' and xpos[4:5] in ('z', 'c', 'g'):\n",
    "        result_pos2features.update({'xpos_mood': xpos[4:5]})\n",
    "    elif xpos[0] == 'P' and xpos[2:3] in ('e', 'a', 'l', 'm', 'q', 't'):\n",
    "        result_pos2features.update({'xpos_ref_type': xpos[2:3]})\n",
    "    elif xpos[0] == 'A' and xpos[4:5] == 'e':\n",
    "        result_pos2features.update({'xpos_extended': xpos[4:5]})\n",
    "    \n",
    "    return {(word + '_' + key): value.rstrip('-') for key, value in result_pos2features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "177d6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries) and an index of a word (dictionary)\n",
    "# output - list of dictionaries (features, for each word)\n",
    "def word2features(sentence, i):\n",
    "    sentence_contains_interrogative_word = contains_interrogative_word(sentence)\n",
    "    sentence_contains_interrogative_particle = contains_interrogative_particle(sentence)\n",
    "    sentence_contains_imperative_verb = contains_imperative_verb(sentence)\n",
    "\n",
    "    features = {\n",
    "        'word': sentence[i]['text'].lower(),\n",
    "        'sent_len': len(sentence),\n",
    "        'upos': sentence[i]['upos'],\n",
    "        'first_word_in_sent': sentence[1]['text'].lower(),\n",
    "        'contains_interrogative_word': sentence_contains_interrogative_word,\n",
    "        'contains_interrogative_particle': sentence_contains_interrogative_particle,\n",
    "        'contains_imperative_verb': sentence_contains_imperative_verb,\n",
    "        'contains_repetitive_conj_before': contains_rep_conj_before(sentence, i),\n",
    "        'between_two_verbs': between_two_verbs(sentence, i),\n",
    "        'contains_publicistic_word': contains_publicistic_word(sentence)\n",
    "    }\n",
    "\n",
    "    features.update(split_xpos(sentence[i]['xpos']))\n",
    "\n",
    "    if i > 0:\n",
    "        features.update({\n",
    "            'prev_word': sentence[i-1]['text'].lower(),\n",
    "            'prev_word_upos': sentence[i-1]['upos']\n",
    "        })\n",
    "\n",
    "        features.update(split_xpos(sentence[i-1]['xpos'], 'prev_word'))\n",
    "    else:\n",
    "        features.update({\n",
    "            'BOS': True\n",
    "        })\n",
    "\n",
    "    if i > 1:\n",
    "        features.update({\n",
    "            'word_before_prev_word': sentence[i-2]['text'].lower(),\n",
    "            'word_before_prev_word_upos': sentence[i-2]['upos']\n",
    "        })\n",
    "        \n",
    "        features.update(split_xpos(sentence[i-2]['xpos'], 'word_before_prev_word'))\n",
    "\n",
    "    if i < len(sentence)-1:\n",
    "        features.update({\n",
    "            'next_word': sentence[i+1]['text'].lower(),\n",
    "            'next_word_upos': sentence[i+1]['upos']\n",
    "        })\n",
    "        \n",
    "        features.update(split_xpos(sentence[i+1]['xpos'], 'next_word'))\n",
    "    else:\n",
    "        features.update({\n",
    "            'EOS': True\n",
    "        })\n",
    "        \n",
    "    if i < len(sentence)-2:\n",
    "        features.update({\n",
    "            'word_after_next_word': sentence [i+2]['text'].lower(),\n",
    "            'word_after_next_word_upos': sentence[i+2]['upos']\n",
    "        })\n",
    "        \n",
    "        features.update(split_xpos(sentence[i+2]['xpos'], 'word_after_next_word'))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b37c7123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries)\n",
    "# output - list of features (dictionaries, for each word)\n",
    "def sent2features(sentence):\n",
    "    return [word2features(sentence, i) for i in range(len(sentence))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3eefcddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - JSON-serializable data and name of the output JSON file\n",
    "# output - None, saves the sentences to a JSON file\n",
    "def save_as_json(data, output_file_name):\n",
    "    with open(output_file_name, \"w\") as output_file:\n",
    "        json.dump(data, output_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc90be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - JSON file\n",
    "# output - the contents of the JSON file as an object\n",
    "def load_json(json_file_name):\n",
    "    with open(json_file_name, \"r\") as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd987f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - name of a text file with one sentence per line and a variable indicating whether or not to save X and y to JSON\n",
    "# output - X and y - features and labels\n",
    "def data_prep(input_file_name, json_serialize=False):\n",
    "    data = add_special_symbol_at_start_of_each_sentence(read_file_as_list_of_sentences(input_file_name))\n",
    "    nlp_tokenize = classla.Pipeline('bg', processors='tokenize')\n",
    "    tokenized_data = run_through_classla_pipeline(data, nlp_tokenize)\n",
    "    \n",
    "    if len(data) != len(tokenized_data):\n",
    "        print(\"Warning: Mismatch in the count of the data and tokenized data\")\n",
    "\n",
    "    squashed_tokenized_data = [squash_punctuation(sentence) for sentence in tokenized_data]\n",
    "\n",
    "    if len(tokenized_data) != len(squashed_tokenized_data):\n",
    "        print(\"Warning: Mismatch in the count of the tokenized and squashed tokenized data\")\n",
    "    \n",
    "    y = [sent2labels(sentence) for sentence in squashed_tokenized_data]\n",
    "    \n",
    "    if len(squashed_tokenized_data) != len(y):\n",
    "        print(\"Warning: Mismatch in the count of the squashed tokenized data and labeled data\")\n",
    "    \n",
    "    data_without_punctuation = [remove_punctuation(sentence) for sentence in squashed_tokenized_data]\n",
    "    \n",
    "    if len(data_without_punctuation) != len(y):\n",
    "        print(\"Warning: Mismatch in the count of the data without punctuation and labeled data\")\n",
    "    \n",
    "    nlp_pos_tokenize = classla.Pipeline('bg', processors='tokenize,pos')   \n",
    "    pos_tokenized_data = run_through_classla_pipeline(data_without_punctuation, nlp_pos_tokenize)\n",
    "    \n",
    "    if len(data_without_punctuation) != len(pos_tokenized_data):\n",
    "        print(\"Warning: Mismatch in the count of the data without punctuation and POS tokenized data\")\n",
    "    \n",
    "    X = [sent2features(sentence) for sentence in pos_tokenized_data]\n",
    "    \n",
    "    if len(X) != len(pos_tokenized_data):\n",
    "        print(\"Warning: Mismatch in the count of the prepped data and POS tokenized data\")\n",
    "    \n",
    "    if json_serialize:\n",
    "        save_as_json(X, re.sub('\\.txt', '_X.json', input_file_name))\n",
    "        save_as_json(y, re.sub('\\.txt', '_y.json', input_file_name))\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7608e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - X and y - features and labels\n",
    "# output - None, prints on the screen the features-labels pairs that have length mismatch\n",
    "def verify_prepped_data(X, y):\n",
    "    for feat, label in zip(X, y):\n",
    "        if len(feat) != len(label):\n",
    "            print(feat, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e592e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - X and y - features and labels\n",
    "# output - list of punctuated sentences (strings; y labels applied to X)\n",
    "def punctuate(X, y):\n",
    "    punctuated_sentences = []\n",
    "\n",
    "    for feat, label in zip(X, y):\n",
    "        sentence = ''\n",
    "\n",
    "        for i in range(len(feat)):\n",
    "            sentence = sentence + feat[i]['word'] + label[i] + ' '\n",
    "        \n",
    "        punctuated_sentences.append(sentence.lstrip('^').strip())\n",
    "    \n",
    "    return punctuated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d49d58fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - X and y - features and labels\n",
    "# output - list of punctuated sentences (strings; y labels applied to X)\n",
    "def scikit_punctuate(X, y):\n",
    "    punctuated_sentences = ''\n",
    "\n",
    "    for feat, label in zip(X, y):\n",
    "        if feat['word'] == '^':\n",
    "            pass\n",
    "        else:\n",
    "            punctuated_sentences = punctuated_sentences + feat['word'] + label + ' '\n",
    "    \n",
    "    return punctuated_sentences.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8febe6d7",
   "metadata": {},
   "source": [
    "### NN utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "946190c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - data - a list of lists\n",
    "# output - a numpy array of numpy arrays\n",
    "def transform_to_array(data):\n",
    "    return array([array(sequence) for sequence in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a55aa505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - data - a list of lists of labels (strings)\n",
    "# output - a list of labels (data flattened)\n",
    "def extract_labels(data):\n",
    "    return [label for sentence in data for label in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "662dfe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - labels (list of strings), label_encoder (optional) that transforms string labels to numeric ones\n",
    "# output - the one hot encoded labels (binary matrix) and a label_encoder (if one was created)\n",
    "def one_hot_encode_labels(labels, label_encoder=False):\n",
    "    if label_encoder:\n",
    "        new_labels = label_encoder.transform(labels)\n",
    "    \n",
    "        return to_categorical(new_labels)\n",
    "    else:\n",
    "        label_encoder = LabelEncoder()\n",
    "        new_labels = label_encoder.fit_transform(labels)\n",
    "        \n",
    "        return to_categorical(new_labels), label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "275fd7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - labels (a matrix), label_encoder that is used to detransform numeric labels to string ones\n",
    "# output - list of labels (strings)\n",
    "def inverse_transform_one_hot_encoded_labels(labels, label_encoder):\n",
    "    decoded_labels = [argmax(label, axis=None, out=None) for label in labels]\n",
    "    return list(label_encoder.inverse_transform(decoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5a4221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - data (list of lists of strings), tokenizer (optional) that transforms strings to numbers\n",
    "# output - list of lists of numbers and a tokenizer (if one was created)\n",
    "def transform_text_to_numbers(data, tokenizer=False):\n",
    "    if tokenizer:\n",
    "        return tokenizer.texts_to_sequences(data)\n",
    "    else:\n",
    "        tokenizer = Tokenizer(oov_token='oov')\n",
    "        tokenizer.fit_on_texts(data)\n",
    "\n",
    "        return tokenizer.texts_to_sequences(data), tokenizer        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16f23621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - y_actual (a list of strings - the actual class labels),\n",
    "#       - y_pred (a list of strings - the predicted class labels),\n",
    "#       - label_encoder - to get a list of the classes\n",
    "# output - prints the classification report about F-measure, precision and recall\n",
    "def nn_classification_report(y_actual, y_pred, label_encoder):\n",
    "    labels=list(label_encoder.classes_)\n",
    "    labels.remove('')\n",
    "    print(classification_report(y_actual, list(y_pred), labels=labels, digits=3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c253c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - model (a neural network model),\n",
    "#       - output_file_name_model (string with the file name where to save the model in JSON format)\n",
    "#       - output_file_name_weights (string with the file name where to save the model weights in H5 format)\n",
    "# no output - writes to files\n",
    "def save_nn_model(model, output_file_name_model, output_file_name_weights):\n",
    "    model_json = model.to_json()\n",
    "\n",
    "    with open(output_file_name_model, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "        model.save_weights(output_file_name_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f40a28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - output_file_name_model (string with the file name where the model is saved in JSON format)\n",
    "#       - output_file_name_weights (string with the file name where the model weights are saved in H5 format)\n",
    "# output - a neural network model\n",
    "def load_nn_model(output_file_name_model, output_file_name_weights):\n",
    "    with open(output_file_name_model, 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(output_file_name_weights)\n",
    "    \n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb8ef63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - features (a dictionary)\n",
    "# output - a list with the word feature values for 5 word window\n",
    "def word_features(features):\n",
    "    return [features.get('word_before_prev_word', ''),\n",
    "            features.get('prev_word', ''),\n",
    "            features.get('word', ''),\n",
    "            features.get('next_word', ''),\n",
    "            features.get('word_after_next_word', '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af310b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - xpos subtags values (strings)\n",
    "# output - one xpos string with the concatenated subtags for the particular xpos type\n",
    "def xpos_tags_to_string(xpos_type, xpos_gender_number_article, xpos_mood, xpos_ref_type, xpos_extended):\n",
    "    if xpos_type[0] in ('D', 'C', 'T', 'R', 'I'):\n",
    "        return xpos_type\n",
    "    elif xpos_type[0] in ('N', 'H', 'M'):\n",
    "        return xpos_type + xpos_gender_number_article\n",
    "    elif xpos_type[0] == 'A':\n",
    "        return xpos_type + xpos_gender_number_article + xpos_extended\n",
    "    elif xpos_type[0] == 'P':\n",
    "        return xpos_type + xpos_gender_number_article + xpos_ref_type\n",
    "    elif xpos_type[0] == 'V':\n",
    "        return xpos_type + xpos_gender_number_article + xpos_mood\n",
    "    else:\n",
    "        return xpos_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ae56c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - features (a dictionary), word (a string) - dictionary key prefix\n",
    "# output - a list with xpos subtags for a word\n",
    "def extract_xpos_tags(features, word):\n",
    "    return [features.get(word + 'xpos_type', '-'),\n",
    "            features.get(word + 'xpos_gender_number_article', '-'),\n",
    "            features.get(word + 'xpos_mood', '-'),\n",
    "            features.get(word + 'xpos_ref_type', '-'),\n",
    "            features.get(word + 'xpos_extended', '-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c68f6cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - features (a dictionary)\n",
    "# output - a list of xpos full tags (strings) for 5 word window\n",
    "def xpos_features(features):\n",
    "    return [xpos_tags_to_string(*extract_xpos_tags(features, 'word_before_prev_word_')),\n",
    "            xpos_tags_to_string(*extract_xpos_tags(features, 'prev_word_')),\n",
    "            xpos_tags_to_string(*extract_xpos_tags(features, 'word_')),\n",
    "            xpos_tags_to_string(*extract_xpos_tags(features, 'next_word_')),\n",
    "            xpos_tags_to_string(*extract_xpos_tags(features, 'word_after_next_word_'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2daa8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - features (a dictionary)\n",
    "# output - a list of upos tags (strings) for 5 word window\n",
    "def upos_features(features):\n",
    "    return [features.get('word_before_prev_word_upos', ''),\n",
    "            features.get('prev_word_upos', ''),\n",
    "            features.get('upos', ''),\n",
    "            features.get('next_word_upos', ''),\n",
    "            features.get('word_after_next_word_upos', '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eddfd342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - features (a dictionary)\n",
    "# output - a list of bool and numeric feture values\n",
    "def bool_numeric_features(features):\n",
    "    return [features.get('sent_len'),\n",
    "            int(features.get('contains_interrogative_word')),\n",
    "            int(features.get('contains_interrogative_particle')),\n",
    "            int(features.get('contains_imperative_verb')),\n",
    "            int(features.get('contains_repetitive_conj_before')),\n",
    "            int(features.get('between_two_verbs')),\n",
    "            int(features.get('contains_publicistic_word')),\n",
    "            int(features.get('BOS', False)),\n",
    "            int(features.get('EOS', False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95f97b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - data (a list of lists of dictionaries), function (a function specifying which features to extract)\n",
    "# output - a list of lists\n",
    "def extract_features(data, function):\n",
    "    return [function(word) for sentence in data for word in sentence]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
