{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "167631fb",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f0a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "import classla\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "744a6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - name of a text file with one sentence per line\n",
    "# output - list of sentences (strings)\n",
    "def read_file_as_list_of_sentences(input_file_name):\n",
    "    with open(input_file_name, \"r\") as input_file:\n",
    "        return input_file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bee954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - list of sentences (strings) and a classla pipeline (POS tokenize or tokenize)\n",
    "# output - list of dictionaries ((POS) tokenized sentences) \n",
    "def run_through_classla_pipeline(list_of_sentences, pipeline):\n",
    "    return [pipeline(sentence).to_dict()[0][0] for sentence in list_of_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2500246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - list of dictionaries (a dictionary for each word - the tokenized version of the sentence)\n",
    "# output - list of dictionaries (dictionaries that contain punctuation one after another are squashed into one)\n",
    "def squash_punctuation(sentence):\n",
    "    new_sentence = []\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i]['text'] in ',()\"[];.?!:-':\n",
    "            if len(new_sentence) > 0 and all(character in ',()\"[];.?!:-' for character in new_sentence[-1]['text']):\n",
    "                new_sentence[-1]['text'] += sentence[i]['text']\n",
    "            else:\n",
    "                new_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_sentence.append(sentence[i])\n",
    "            \n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53f8f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence and an index of a word (dictionary) - assigns a label to each word based on the punctuation after\n",
    "# output - a label - None for punctuation, punctuation for words followed by punctuation and empty for words if they are not\n",
    "def word2label(sentence, i):\n",
    "    if all(character in ',()\"[];.?!:-' for character in sentence[i]['text']):\n",
    "        return None\n",
    "\n",
    "    if i < len(sentence) - 1:\n",
    "        if all(character in ',()\"[];.?!:-' for character in sentence[i+1]['text']):\n",
    "            label = sentence[i+1]['text']\n",
    "            return label\n",
    "    \n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ed05a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries)\n",
    "# output - list of labels (strings) with None labels for punctuation being filtered out\n",
    "def sent2labels(sentence):\n",
    "    return [label for label in (word2label(sentence, i) for i in range(len(sentence))) if label != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f1f7fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - list of dictionaries (a dictionary for each word - the tokenized version of the sentence)\n",
    "# output - new sentence (string) with the punctuation removed\n",
    "def remove_punctuation(sentence):\n",
    "    new_sentence = ''\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        if all(character in ',()\"[];.?!:-' for character in sentence[i]['text']):\n",
    "            pass\n",
    "        else:\n",
    "            new_sentence = new_sentence + sentence[i]['text'] + ' '\n",
    "    \n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdaf90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries) and an index of a word (dictionary)\n",
    "# output - list of dictionaries (features, for each word)\n",
    "def word2features(sentence, i):\n",
    "    features = {\n",
    "        'word': sentence[i]['text'],\n",
    "        'sent_len': len(sentence),\n",
    "        'pos_in_sent': i,\n",
    "        'upos': sentence[i]['upos'],\n",
    "        'xpos': sentence[i]['xpos'],\n",
    "        'first_word_in_sent': sentence[0]['text']\n",
    "    }\n",
    "\n",
    "    if i > 0:\n",
    "        features.update({\n",
    "            'prev_word': sentence[i-1]['text']\n",
    "        })\n",
    "    else:\n",
    "        features.update({\n",
    "            'BOS': True\n",
    "        })\n",
    "\n",
    "    if i < len(sentence)-1:\n",
    "        features.update({\n",
    "            'next_word': sentence[i+1]['text']\n",
    "        })\n",
    "    else:\n",
    "        features.update({\n",
    "            'EOS': True\n",
    "        })\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2a093df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries)\n",
    "# output - list of features (dictionaries, for each word)\n",
    "def sent2features(sentence):\n",
    "    return [word2features(sentence, i) for i in range(len(sentence))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0f67be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - JSON-serializable data and name of the output JSON file\n",
    "# output - None, saves the sentences to a JSON file\n",
    "def save_as_json(data, output_file_name):\n",
    "    with open(output_file_name, \"w\") as output_file:\n",
    "        json.dump(data, output_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14f2771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - name of a text file with one sentence per line and a variable indicating whether or not to save X and y to JSON\n",
    "# output - X and y - features and labels\n",
    "def data_prep(input_file_name, json_serialize=False):\n",
    "    data = read_file_as_list_of_sentences(input_file_name)\n",
    "    nlp_tokenize = classla.Pipeline('bg', processors='tokenize')\n",
    "    tokenized_data = run_through_classla_pipeline(data, nlp_tokenize)\n",
    "    \n",
    "    if len(data) != len(tokenized_data):\n",
    "        print(\"Warning: Mismatch in the count of the data and tokenized data\")\n",
    "\n",
    "    squashed_tokenized_data = [squash_punctuation(sentence) for sentence in tokenized_data]\n",
    "\n",
    "    if len(tokenized_data) != len(squashed_tokenized_data):\n",
    "        print(\"Warning: Mismatch in the count of the tokenized and squashed tokenized data\")\n",
    "    \n",
    "    y = [sent2labels(sentence) for sentence in squashed_tokenized_data]\n",
    "    \n",
    "    if len(squashed_tokenized_data) != len(y):\n",
    "        print(\"Warning: Mismatch in the count of the squashed tokenized data and labeled data\")\n",
    "    \n",
    "    data_without_punctuation = [remove_punctuation(sentence) for sentence in squashed_tokenized_data]\n",
    "    \n",
    "    if len(data_without_punctuation) != len(y):\n",
    "        print(\"Warning: Mismatch in the count of the data without punctuation and labeled data\")\n",
    "    \n",
    "    nlp_pos_tokenize = classla.Pipeline('bg', processors='tokenize,pos')   \n",
    "    pos_tokenized_data = run_through_classla_pipeline(data_without_punctuation, nlp_pos_tokenize)\n",
    "    \n",
    "    if len(data_without_punctuation) != len(pos_tokenized_data):\n",
    "        print(\"Warning: Mismatch in the count of the data without punctuation and POS tokenized data\")\n",
    "    \n",
    "    X = [sent2features(sentence) for sentence in pos_tokenized_data]\n",
    "    \n",
    "    if len(X) != len(pos_tokenized_data):\n",
    "        print(\"Warning: Mismatch in the count of the prepped data and POS tokenized data\")\n",
    "    \n",
    "    if json_serialize:\n",
    "        save_as_json(X, re.sub('\\.txt', '_X.txt', input_file_name))\n",
    "        save_as_json(y, re.sub('\\.txt', '_y.txt', input_file_name))\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0b29aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - X and y - features and labels\n",
    "# output - None, prints on the screen the features-labels pairs that have length mismatch\n",
    "def verify_prepped_data(X, y):\n",
    "    for feat, label in zip(X, y):\n",
    "        if len(feat) != len(label):\n",
    "            print(feat, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a2311f",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccfe02a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-25 21:11:17 INFO: Loading these models for language: bg (Bulgarian):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | standard |\n",
      "========================\n",
      "\n",
      "2021-09-25 21:11:17 INFO: Use device: gpu\n",
      "2021-09-25 21:11:17 INFO: Loading: tokenize\n",
      "2021-09-25 21:11:17 INFO: Done loading processors!\n",
      "2021-09-25 21:11:20 INFO: Loading these models for language: bg (Bulgarian):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | standard |\n",
      "| pos       | standard |\n",
      "========================\n",
      "\n",
      "2021-09-25 21:11:20 INFO: Use device: gpu\n",
      "2021-09-25 21:11:20 INFO: Loading: tokenize\n",
      "2021-09-25 21:11:20 INFO: Loading: pos\n",
      "2021-09-25 21:11:25 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 40s, sys: 1.82 s, total: 5min 41s\n",
      "Wall time: 5min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, y_train = data_prep('../data/Bible/processed/Bibliia_clean_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb210480",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_prepped_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8ff7528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-25 21:21:48 INFO: Loading these models for language: bg (Bulgarian):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | standard |\n",
      "========================\n",
      "\n",
      "2021-09-25 21:21:48 INFO: Use device: gpu\n",
      "2021-09-25 21:21:48 INFO: Loading: tokenize\n",
      "2021-09-25 21:21:48 INFO: Done loading processors!\n",
      "2021-09-25 21:21:49 INFO: Loading these models for language: bg (Bulgarian):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | standard |\n",
      "| pos       | standard |\n",
      "========================\n",
      "\n",
      "2021-09-25 21:21:49 INFO: Use device: gpu\n",
      "2021-09-25 21:21:49 INFO: Loading: tokenize\n",
      "2021-09-25 21:21:49 INFO: Loading: pos\n",
      "2021-09-25 21:21:50 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min, sys: 404 ms, total: 3min\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_dev, y_dev = data_prep('../data/Bible/processed/Bibliia_clean_dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d8ee44",
   "metadata": {},
   "source": [
    "### CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b055193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 15s, sys: 95.9 ms, total: 2min 15s\n",
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0f0c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80536204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ':',\n",
       " '!',\n",
       " ',',\n",
       " '?',\n",
       " ';',\n",
       " '.',\n",
       " '-',\n",
       " ',(',\n",
       " '\"',\n",
       " '\").',\n",
       " '(',\n",
       " ')',\n",
       " '?-',\n",
       " '.-',\n",
       " ':)',\n",
       " '!-',\n",
       " ':\"',\n",
       " '.\"',\n",
       " ':(',\n",
       " '?)',\n",
       " '\"?',\n",
       " '\".',\n",
       " '\";',\n",
       " '\"-',\n",
       " ').',\n",
       " '!,',\n",
       " '-\"',\n",
       " ')!',\n",
       " '\",',\n",
       " '.\")',\n",
       " '!;',\n",
       " ';(',\n",
       " '),',\n",
       " '?;',\n",
       " '?\"',\n",
       " ');',\n",
       " '.)',\n",
       " '?,',\n",
       " ';\"',\n",
       " '):']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "392346a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02934f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':',\n",
       " '!',\n",
       " ',',\n",
       " '?',\n",
       " ';',\n",
       " '.',\n",
       " '-',\n",
       " ',(',\n",
       " '\"',\n",
       " '\").',\n",
       " '(',\n",
       " ')',\n",
       " '?-',\n",
       " '.-',\n",
       " ':)',\n",
       " '!-',\n",
       " ':\"',\n",
       " '.\"',\n",
       " ':(',\n",
       " '?)',\n",
       " '\"?',\n",
       " '\".',\n",
       " '\";',\n",
       " '\"-',\n",
       " ').',\n",
       " '!,',\n",
       " '-\"',\n",
       " ')!',\n",
       " '\",',\n",
       " '.\")',\n",
       " '!;',\n",
       " ';(',\n",
       " '),',\n",
       " '?;',\n",
       " '?\"',\n",
       " ');',\n",
       " '.)',\n",
       " '?,',\n",
       " ';\"',\n",
       " '):']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4675c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c123899d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mariya/anaconda3/envs/masters/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1465: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6386622718017901"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.flat_f1_score(y_dev, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "968347ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mariya/anaconda3/envs/masters/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=['!', '\"', '(', ')', ',', '-', '.', ':', ';', '?', ')!', '-\"', '.\"', ':\"', ';\"', '?\"', '.\")', ',(', ':(', ';(', '.)', ':)', '?)', '\").', '!,', '\",', '),', '?,', '!-', '\"-', '.-', '?-', '\".', ').', '):', '!;', '\";', ');', '?;', '\"?'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/mariya/anaconda3/envs/masters/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mariya/anaconda3/envs/masters/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           !      0.533     0.189     0.279       297\n",
      "           \"      0.000     0.000     0.000         3\n",
      "           (      0.000     0.000     0.000         8\n",
      "           )      0.000     0.000     0.000         4\n",
      "           ,      0.738     0.546     0.628      6602\n",
      "           -      0.381     0.054     0.094       298\n",
      "           .      0.842     0.976     0.904      2675\n",
      "           :      0.904     0.710     0.795       799\n",
      "           ;      0.171     0.021     0.038       562\n",
      "           ?      0.717     0.229     0.347       310\n",
      "          )!      0.000     0.000     0.000         1\n",
      "          -\"      0.000     0.000     0.000         1\n",
      "          .\"      0.000     0.000     0.000        18\n",
      "          :\"      0.500     0.077     0.133        26\n",
      "          ;\"      0.000     0.000     0.000         0\n",
      "          ?\"      0.000     0.000     0.000         0\n",
      "         .\")      0.000     0.000     0.000         0\n",
      "          ,(      0.000     0.000     0.000         4\n",
      "          :(      0.000     0.000     0.000         1\n",
      "          ;(      0.000     0.000     0.000         0\n",
      "          .)      0.000     0.000     0.000         1\n",
      "          :)      0.000     0.000     0.000         0\n",
      "          ?)      0.000     0.000     0.000         0\n",
      "         \").      0.000     0.000     0.000         0\n",
      "          !,      0.000     0.000     0.000         0\n",
      "          \",      0.000     0.000     0.000         6\n",
      "          ),      0.000     0.000     0.000         4\n",
      "          ?,      0.000     0.000     0.000         1\n",
      "          !-      0.000     0.000     0.000        20\n",
      "          \"-      0.000     0.000     0.000         2\n",
      "          .-      0.000     0.000     0.000         1\n",
      "          ?-      0.000     0.000     0.000         4\n",
      "          \".      0.000     0.000     0.000         1\n",
      "          ).      0.000     0.000     0.000         0\n",
      "          ):      0.000     0.000     0.000         0\n",
      "          !;      0.000     0.000     0.000         1\n",
      "          \";      0.000     0.000     0.000         2\n",
      "          );      0.000     0.000     0.000         0\n",
      "          ?;      0.000     0.000     0.000         0\n",
      "          \"?      0.000     0.000     0.000         1\n",
      "\n",
      "   micro avg      0.776     0.596     0.674     11653\n",
      "   macro avg      0.120     0.070     0.080     11653\n",
      "weighted avg      0.725     0.596     0.639     11653\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show all metrics\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_dev, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f302345",
   "metadata": {},
   "source": [
    "### Grid Search Try - Worse Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a7510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import make_scorer\n",
    "# import scipy.stats\n",
    "# from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70add66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # define fixed parameters and parameters to search\n",
    "# crf = sklearn_crfsuite.CRF(\n",
    "#     algorithm='lbfgs',\n",
    "#     max_iterations=100,\n",
    "#     all_possible_transitions=True\n",
    "# )\n",
    "# params_space = {\n",
    "#     'c1': scipy.stats.expon(scale=0.5),\n",
    "#     'c2': scipy.stats.expon(scale=0.05),\n",
    "# }\n",
    "\n",
    "# # use the same metric for evaluation\n",
    "# f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "#                         average='weighted', labels=labels)\n",
    "\n",
    "# # search\n",
    "# rs = RandomizedSearchCV(crf, params_space,\n",
    "#                         cv=3,\n",
    "#                         verbose=1,\n",
    "#                         n_jobs=-1,\n",
    "#                         n_iter=50,\n",
    "#                         scoring=f1_scorer)\n",
    "# rs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf = rs.best_estimator_\n",
    "# print('best params:', rs.best_params_)\n",
    "# print('best CV score:', rs.best_score_)\n",
    "# print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac24a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf = rs.best_estimator_\n",
    "# y_pred = crf.predict(X_test)\n",
    "\n",
    "# sorted_labels = sorted(\n",
    "#     labels,\n",
    "#     key=lambda name: (name[1:], name[0])\n",
    "# )\n",
    "\n",
    "# print(metrics.flat_classification_report(\n",
    "#     y_test, y_pred, labels=sorted_labels, digits=3\n",
    "# ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
