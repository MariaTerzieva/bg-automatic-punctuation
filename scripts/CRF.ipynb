{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f0a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "import classla\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "744a6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - name of a text file with one sentence per line\n",
    "# output - list of sentences (strings)\n",
    "def read_file_as_list_of_sentences(input_file_name):\n",
    "    with open(input_file_name, \"r\") as input_file:\n",
    "        return input_file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bee954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - list of sentences (strings) and a classla pipeline (POS tokenize or tokenize)\n",
    "# output - list of dictionaries ((POS) tokenized sentences) \n",
    "def run_through_classla_pipeline(list_of_sentences, pipeline):\n",
    "    return [pipeline(sentence).to_dict()[0][0] for sentence in list_of_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2500246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - list of dictionaries (a dictionary for each word - the tokenized version of the sentence)\n",
    "# output - list of dictionaries (dictionaries that contain punctuation one after another are squashed into one)\n",
    "def squash_punctuation(sentence):\n",
    "    new_sentence = []\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i]['text'] in ',()\"[];.?!:-':\n",
    "            if len(new_sentence) > 0 and all(character in ',()\"[];.?!:-' for character in new_sentence[-1]['text']):\n",
    "                new_sentence[-1]['text'] += sentence[i]['text']\n",
    "            else:\n",
    "                new_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_sentence.append(sentence[i])\n",
    "            \n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53f8f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence and an index of a word (dictionary) - assigns a label to each word based on the punctuation after\n",
    "# output - a label - None for punctuation, punctuation for words followed by punctuation and empty for words if they are not\n",
    "def word2label(sentence, i):\n",
    "    if all(character in ',()\"[];.?!:-' for character in sentence[i]['text']):\n",
    "        return None\n",
    "\n",
    "    if i < len(sentence) - 1:\n",
    "        if all(character in ',()\"[];.?!:-' for character in sentence[i+1]['text']):\n",
    "            label = sentence[i+1]['text']\n",
    "            return label\n",
    "    \n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ed05a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries)\n",
    "# output - list of labels (strings) with None labels for punctuation being filtered out\n",
    "def sent2labels(sentence):\n",
    "    return [label for label in (word2label(sentence, i) for i in range(len(sentence))) if label != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f1f7fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - list of dictionaries (a dictionary for each word - the tokenized version of the sentence)\n",
    "# output - new sentence (string) with the punctuation removed\n",
    "def remove_punctuation(sentence):\n",
    "    new_sentence = ''\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        if all(character in ',()\"[];.?!:-' for character in sentence[i]['text']):\n",
    "            pass\n",
    "        else:\n",
    "            new_sentence = new_sentence + sentence[i]['text'] + ' '\n",
    "    \n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdaf90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries) and an index of a word (dictionary)\n",
    "# output - list of dictionaries (features, for each word)\n",
    "def word2features(sentence, i):\n",
    "    features = {\n",
    "        'word': sentence[i]['text'],\n",
    "        'sent_len': len(sentence),\n",
    "        'pos_in_sent': i,\n",
    "        'upos': sentence[i]['upos'],\n",
    "        'xpos': sentence[i]['xpos'],\n",
    "        'first_word_in_sent': sentence[0]['text']\n",
    "    }\n",
    "\n",
    "    if i > 0:\n",
    "        features.update({\n",
    "            'prev_word': sentence[i-1]['text']\n",
    "        })\n",
    "    else:\n",
    "        features.update({\n",
    "            'BOS': True\n",
    "        })\n",
    "\n",
    "    if i < len(sentence)-1:\n",
    "        features.update({\n",
    "            'next_word': sentence[i+1]['text']\n",
    "        })\n",
    "    else:\n",
    "        features.update({\n",
    "            'EOS': True\n",
    "        })\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2a093df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - sentence (list of dictionaries)\n",
    "# output - list of features (dictionaries, for each word)\n",
    "def sent2features(sentence):\n",
    "    return [word2features(sentence, i) for i in range(len(sentence))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0f67be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - a list of lists of dictionaries ((POS) tokenized sentences) and name of the output JSON file\n",
    "# output - None, saves the sentences to a JSON file\n",
    "def save_pos_tokenized_sentences_as_json(pos_tokenized_sentences, output_file_name):\n",
    "    with open(output_file_name, \"w\") as output_file:\n",
    "        json.dump(pos_tokenized_sentences, output_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3c8ee654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - name of a text file with one sentence per line\n",
    "# output - X and y - features and labels\n",
    "def data_prep(input_file_name):\n",
    "    data = read_file_as_list_of_sentences(input_file_name)\n",
    "    nlp_tokenize = classla.Pipeline('bg', processors='tokenize')\n",
    "    tokenized_data = run_through_classla_pipeline(data, nlp_tokenize)\n",
    "    \n",
    "    if len(data) != len(tokenized_data):\n",
    "        print(\"Warning: Mismatch in the count of the data and tokenized data\")\n",
    "\n",
    "    squashed_tokenized_data = [squash_punctuation(sentence) for sentence in tokenized_data]\n",
    "\n",
    "    if len(tokenized_data) != len(squashed_tokenized_data):\n",
    "        print(\"Warning: Mismatch in the count of the tokenized and squashed tokenized data\")\n",
    "    \n",
    "    y = [sent2labels(sentence) for sentence in squashed_tokenized_data]\n",
    "    \n",
    "    if len(squashed_tokenized_data) != len(y):\n",
    "        print(\"Warning: Mismatch in the count of the squashed tokenized data and labeled data\")\n",
    "    \n",
    "    data_without_punctuation = [remove_punctuation(sentence) for sentence in squashed_tokenized_data]\n",
    "    \n",
    "    if len(data_without_punctuation) != len(y):\n",
    "        print(\"Warning: Mismatch in the count of the data without punctuation and labeled data\")\n",
    "    \n",
    "    nlp_pos_tokenize = classla.Pipeline('bg', processors='tokenize,pos')   \n",
    "    pos_tokenized_data = run_through_classla_pipeline(data_without_punctuation, nlp_pos_tokenize)\n",
    "    \n",
    "    if len(data_without_punctuation) != len(pos_tokenized_data):\n",
    "        print(\"Warning: Mismatch in the count of the data without punctuation and POS tokenized data\")\n",
    "    \n",
    "    X = [sent2features(sentence) for sentence in pos_tokenized_data]\n",
    "    \n",
    "    if len(X) != len(pos_tokenized_data):\n",
    "        print(\"Warning: Mismatch in the count of the prepped data and POS tokenized data\")\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "47cade03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - X and y - features and labels\n",
    "# output - None, prints on the screen the features-labels pairs that have length mismatch\n",
    "def verify_prepped_data(X, y):\n",
    "    for feat, label in zip(X, y):\n",
    "        if len(feat) != len(label):\n",
    "            print(feat, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ac024447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-22 20:04:25 INFO: Loading these models for language: bg (Bulgarian):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | standard |\n",
      "========================\n",
      "\n",
      "2021-09-22 20:04:25 INFO: Use device: gpu\n",
      "2021-09-22 20:04:25 INFO: Loading: tokenize\n",
      "2021-09-22 20:04:25 INFO: Done loading processors!\n",
      "2021-09-22 20:04:26 INFO: Loading these models for language: bg (Bulgarian):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | standard |\n",
      "| pos       | standard |\n",
      "========================\n",
      "\n",
      "2021-09-22 20:04:26 INFO: Use device: gpu\n",
      "2021-09-22 20:04:26 INFO: Loading: tokenize\n",
      "2021-09-22 20:04:26 INFO: Loading: pos\n",
      "2021-09-22 20:04:27 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 53s, sys: 442 ms, total: 2min 53s\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, y = data_prep('../data/Bible/processed/Bibliia_clean_dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b4da698",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_prepped_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a8ff7528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-22 20:50:13 INFO: Loading these models for language: bg (Bulgarian):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | standard |\n",
      "========================\n",
      "\n",
      "2021-09-22 20:50:13 INFO: Use device: gpu\n",
      "2021-09-22 20:50:13 INFO: Loading: tokenize\n",
      "2021-09-22 20:50:13 INFO: Done loading processors!\n",
      "2021-09-22 20:50:15 INFO: Loading these models for language: bg (Bulgarian):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | standard |\n",
      "| pos       | standard |\n",
      "========================\n",
      "\n",
      "2021-09-22 20:50:15 INFO: Use device: gpu\n",
      "2021-09-22 20:50:15 INFO: Loading: tokenize\n",
      "2021-09-22 20:50:15 INFO: Loading: pos\n",
      "2021-09-22 20:50:16 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 17s, sys: 314 ms, total: 2min 17s\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test, y_test = data_prep('../data/Bible/processed/Bibliia_clean_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1b055193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.9 s, sys: 14.2 ms, total: 21.9 s\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a0f0c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "80536204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ',',\n",
       " ';',\n",
       " '.',\n",
       " '-',\n",
       " '!',\n",
       " '?',\n",
       " ':',\n",
       " ':\"',\n",
       " '\"?',\n",
       " '!-',\n",
       " '.\"',\n",
       " '(',\n",
       " ')',\n",
       " '\";',\n",
       " '),',\n",
       " '?-',\n",
       " '.-',\n",
       " ').',\n",
       " ',(',\n",
       " ':(',\n",
       " '!\"',\n",
       " ',)',\n",
       " '\"',\n",
       " '\",',\n",
       " '!;',\n",
       " '?\"',\n",
       " '?)',\n",
       " '.)']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "392346a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "02934f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " ';',\n",
       " '.',\n",
       " '-',\n",
       " '!',\n",
       " '?',\n",
       " ':',\n",
       " ':\"',\n",
       " '\"?',\n",
       " '!-',\n",
       " '.\"',\n",
       " '(',\n",
       " ')',\n",
       " '\";',\n",
       " '),',\n",
       " '?-',\n",
       " '.-',\n",
       " ').',\n",
       " ',(',\n",
       " ':(',\n",
       " '!\"',\n",
       " ',)',\n",
       " '\"',\n",
       " '\",',\n",
       " '!;',\n",
       " '?\"',\n",
       " '?)',\n",
       " '.)']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4675c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c123899d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mariya/anaconda3/envs/masters/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1496: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6191757607661913"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "53109182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mariya/anaconda3/envs/masters/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass labels=['!', '\"', '(', ')', ',', '-', '.', ':', ';', '?', '!\"', '.\"', ':\"', '?\"', ',(', ':(', ',)', '.)', '?)', '\",', '),', '!-', '.-', '?-', ').', '!;', '\";', '\"?'] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/home/mariya/anaconda3/envs/masters/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mariya/anaconda3/envs/masters/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           !      0.478     0.110     0.178       292\n",
      "           \"      0.000     0.000     0.000         4\n",
      "           (      0.000     0.000     0.000         7\n",
      "           )      0.000     0.000     0.000         4\n",
      "           ,      0.729     0.520     0.607      6558\n",
      "           -      0.452     0.081     0.137       346\n",
      "           .      0.843     0.983     0.908      2698\n",
      "           :      0.927     0.652     0.766       784\n",
      "           ;      0.150     0.018     0.032       665\n",
      "           ?      0.730     0.249     0.372       293\n",
      "          !\"      0.000     0.000     0.000         1\n",
      "          .\"      0.000     0.000     0.000        14\n",
      "          :\"      0.500     0.087     0.148        23\n",
      "          ?\"      0.000     0.000     0.000         2\n",
      "          ,(      0.000     0.000     0.000         3\n",
      "          :(      0.000     0.000     0.000         0\n",
      "          ,)      0.000     0.000     0.000         0\n",
      "          .)      0.000     0.000     0.000         1\n",
      "          ?)      0.000     0.000     0.000         0\n",
      "          \",      0.000     0.000     0.000         8\n",
      "          ),      0.000     0.000     0.000         3\n",
      "          !-      0.000     0.000     0.000        17\n",
      "          .-      0.000     0.000     0.000         2\n",
      "          ?-      0.000     0.000     0.000         5\n",
      "          ).      0.000     0.000     0.000         1\n",
      "          !;      0.000     0.000     0.000         0\n",
      "          \";      0.000     0.000     0.000         1\n",
      "          \"?      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.773     0.573     0.658     11732\n",
      "   macro avg      0.172     0.096     0.112     11732\n",
      "weighted avg      0.716     0.573     0.619     11732\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mariya/anaconda3/envs/masters/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mariya/anaconda3/envs/masters/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mariya/anaconda3/envs/masters/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mariya/anaconda3/envs/masters/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e718ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
